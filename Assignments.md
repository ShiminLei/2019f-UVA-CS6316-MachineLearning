---
layout: page
title: Assignments
desc: "Information of Assignments and Final Project for 2019 Fall UVa CS 6316 Machine Learning"
---

<hr>

### Six assignments (60%)
+ Post in collab 
+ You will receive grading of each HWs within 10 day of each due time. A release email will send to you about the grading. (If you don’t receive such emails in time, please do email to to <br>
[instructors-19f-cs-6316@collab.its.virginia.edu](mailto:instructors-19f-cs-6316@collab.its.virginia.edu).

+ Please submit all extension requests, questions, and late assignments  to <br>
[instructors-19f-cs-6316@collab.its.virginia.edu](mailto:instructors-19f-cs-6316@collab.its.virginia.edu).
 

<table id="datatab3" summary="Six Assignments" border="1">
<tr>
 <h3><b>
  <th>Index</th>
  <th>Assignment</th>
  <th>Out Date</th>
  <th>In Date</th>
  <th>About</th>
  </b>
  </h3>
</tr>
<tr>
  <td>HW1</td>
  <td>Out in Collab </td>
  <td>W2</td>
  <td>W4</td>
  <td>Linear Regression and Four Optimization to Code</td>
</tr>
<tr>
  <td>HW2</td>
  <td>Out in Collab</td>
  <td>W4</td>
  <td>W6</td>
  <td>Polynomial and Ridge to implement</td>
</tr>
<tr>
  <td>HW3</td>
  <td>Out in Collab</td>
  <td>W7</td>
  <td>W9</td>
  <td>kNN and SVM to implement and compete</td>
</tr>
<tr>
  <td>HW4</td>
  <td>Out in Collab</td>
  <td>W9</td>
  <td>W11</td>
  <td>Deep NN to implement </td>
</tr>
<tr>
  <td>HW5</td>
  <td></td>
  <td>W12</td>
  <td>W14</td>
  <td></td>
</tr>
<tr>
  <td>HW6</td>
  <td></td>
  <td>W14</td>
  <td>W16</td>
  <td></td>
</tr>
</table>

<hr>

### About ten in-class Quizzess (20%)
+ Quizz dates will show on the schedule page
+ Mostly quizzes will be on Mondays
+ Each quizz includes contents we cover in the previous week
+ We will use your top-10 quizzes to calculate the 20%. 

+ Here is the potential paper list: 


|INDEX     | Quiz |
|------|----------------------------|
| Q0   | [URL](https://qiyanjun.github.io/2019f-UVA-CS6316-MachineLearning//Lectures/Q0.pdf) |
| Q1   | [URL](https://forms.gle/3TAzS5Gq4KsfVYzC8) |
| Q2   | [URL](https://forms.gle/bo9mTo1Nor52wtVc6) |
| Q3   | [URL](https://forms.gle/jMAaFFxsZ38ttQQ49) |
| Q4   | [URL](https://forms.gle/4tTNpD4hvUPNtyNq9) |
| Q5   |  [URL](https://forms.gle/zPqSXCZRKrYHVx4b8) |
| Q6   | [URL](https://forms.gle/mVBdT3LLnjvdSqFf7) |
| Q7   | [URL](https://forms.gle/uoipgqGbjV5BsrHH7) |
| Q8   | [URL](https://forms.gle/pUFSQix4eNtyT9zA7) |
| Q9   |  |
| Q10   |  |
| Q11   |  |
| Q12   |  |



### About Final Project (20%)
+ Each team includes up to four students 
+ To understand, reproduce and present one cutting-edge machine learning paper

+ Each team is required to submit three documents for their project
  - 5 Points: A powerpoint file (Due in Collab on Oct 20th) summarizing the paper via a template
  - 3 Points: The updated powerpoint file (Due in Collab on Nov 30th) summarizing the paper via a template and describing the results you reproduce
  - 7 Points: A iPython Jupyter notebook (Due in Collab on Dec 7th) to present the code, data visualization, and to obtain the results and analysis through step by step code cell run. Your team will go through and show the notebook at the final project presentation meeting to the instructors. 
  - 5 Points: A formal presentation to the instructors (in the last week of the semester), presenting your slides and your iPython notebook. 

+ For the iPython Jupyter notebook your team needs to make: 
  - A Jupyter iPython template is shared to help you structure the project code. 
  - Please read the following papers and then make your IPython Jupiter notebook: [Ten Simple Rules for Reproducible Research in Jupyter Notebooks](https://arxiv.org/abs/1810.08055)


+ By Week3, we will use a google spreadsheet to coordinating team forming and paper bidding. 
+ Please discuss with your fellow classmates, forming potential teams ASAP. 
+ We allow self-selected papers. 
+ Please share questions and concerns to  to <br>
[instructors-19f-cs-6316@collab.its.virginia.edu](mailto:instructors-19f-cs-6316@collab.its.virginia.edu). 

+ Here is the potential paper list: 


|INDEX     |Title  & Link  |Conference|Year|
|------|----------------------------|-----|----------|----|
|1   | [An Empirical Study of Example Forgetting during Deep Neural Network Learning](https://openreview.net/pdf?id=BJlxm30cKm)| ICLR      |2019|
|2   | [ROBUSTNESS May Be at ODDS WITH ACCURACY](https://openreview.net/pdf?id=SyxAb30cY7)|ICLR      |2019|
|3   | [Critical Learning Periods in Deep Networks](https://arxiv.org/abs/1711.08856) |ICLR      |2019|
|4   | [LEARNING ROBUST REPRESENTATIONS BY PROJECTING SUPERFICIAL STATISTICS OUT](https://openreview.net/forum?id=rJEjjoR9K7) |ICLR      |2019|
|5   | [Classification from Positive, Unlabeled and Biased Negative Data](https://openreview.net/pdf?id=H1ldNoC9tX)| ICLR    |2019|
|6   | [Select Via Proxy: Efficient Data Selection For Training Deep Networks](https://openreview.net/pdf?id=ryzHXnR5Y7)| ICLR      |2019|
|7   | [Using Pre-Training Can Improve Model Robustness and Uncertainty](https://arxiv.org/abs/1901.09960) |ICML      |2019|
|8   | [On Learning Invariant Representations for Domain Adaptation](https://arxiv.org/abs/1901.09453)| ICML      |2019|
|9   | [Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks](https://arxiv.org/pdf/1901.08584.pdf)| ICML      |2019|
|10  | [Gradient Descent Finds Global Minima of Deep Neural Networks](https://arxiv.org/abs/1811.03804)| ICML      |2019|
|11  | [When Samples Are Strategically Selected ](https://users.cs.duke.edu/~conitzer/strategicsamples18.pdf) |ICML      |2019|
|12  | [The Odds are Odd: A Statistical Test for Detecting Adversarial Examples](https://arxiv.org/abs/1902.04818)| ICML      |2019|
|13  | [Bias Also Matters: Bias Attribution for Deep Neural Network Explanation ](http://proceedings.mlr.press/v97/wang19p/wang19p.pdf) |ICML      |2019|
|14  | [Escaping Saddle Points with Adaptive Gradient Methods](http://proceedings.mlr.press/v97/staib19a/staib19a.pdf)| ICML      |2019|
|15  | [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)|ICML      |2019|
|16  |[Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913) |NeurIPS      |2018|
|17  | [Modern Neural Networks Generalize on Small Data Sets](http://papers.NeurIPS.cc/paper/7620-modern-neural-networks-generalize-on-small-data-sets.pdf)| NeurIPS      |2018|
|18  | [Generative modeling for protein structures](https://papers.NeurIPS.cc/paper/7978-generative-modeling-for-protein-structures.pdf)| NeurIPS      |2018|
|19  | [On Binary Classification in Extreme Regions](https://papers.NeurIPS.cc/paper/7572-on-binary-classification-in-extreme-regions.pdf)| NeurIPS      |2018|
|20  | [The Description Length of Deep Learning models](https://arxiv.org/abs/1802.07044)| NeurIPS      |2018|
|21  | [L1-regression with Heavy-tailed Distributions](https://arxiv.org/abs/1805.00616)| NeurIPS      |2018|
|22  | [Dynamic Network Model from Partial Observations](https://arxiv.org/abs/1805.10616)| NeurIPS      |2018|
|23  | [Learning Invariances using the Marginal Likelihood](https://arxiv.org/abs/1808.05563) | NeurIPS      |2018|
|24  | [How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective ](https://papers.NeurIPS.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective.pdf)| NeurIPS      |2018|
|25  | [On the Local Minima of the Empirical Risk](https://arxiv.org/abs/1803.09357)| NeurIPS      |2018|
|26  | [Human-in-the-Loop Interpretability Prior](https://arxiv.org/abs/1805.11571)|  NeurIPS      |2018|
|27  | [Processing of missing data by neural networks](https://papers.NeurIPS.cc/paper/7537-processing-of-missing-data-by-neural-networks.pdf)| NeurIPS      |2018|
|28  | [Maximum-Entropy Fine Grained Classification](http://papers.NeurIPS.cc/paper/7344-maximum-entropy-fine-grained-classification.pdf)| NeurIPS      |2018|
|29  | [Deep Structured Prediction with Nonlinear Output Transformations](http://papers.NeurIPS.cc/paper/7869-deep-structured-prediction-with-nonlinear-output-transformations)|NeurIPS      |2018|
|30  | [Large Margin Deep Networks for Classification](http://papers.NeurIPS.cc/paper/7364-large-margin-deep-networks-for-classification)| NeurIPS      |2018|
|31  | [Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation ](http://papers.NeurIPS.cc/paper/8167-towards-understanding-learning-representations-to-what-extent-do-different-neural-networks-learn-the-same-representation)| NeurIPS      |2018|
|32  | [Norm matters: efficient and accurate normalization schemes in deep networks](http://papers.NeurIPS.cc/paper/7485-norm-matters-efficient-and-accurate-normalization-schemes-in-deep-networks)|NeurIPS      |2018|
|33  | [Query K-means Clustering and the Double Dixie Cup Problem](http://papers.NeurIPS.cc/paper/7899-query-k-means-clustering-and-the-double-dixie-cup-problem)| NeurIPS      |2018|
|34  | [Bilevel learning of the Group Lasso structure](https://papers.NeurIPS.cc/paper/8051-bilevel-learning-of-the-group-lasso-structure)| NeurIPS      |2018|
|35  | [Loss Functions for Multiset Prediction](http://papers.NeurIPS.cc/paper/7820-loss-functions-for-multiset-prediction)| NeurIPS      |2018|
|36  | [Active Learning for Non-Parametric Regression Using Purely Random Trees ](http://papers.NeurIPS.cc/paper/7520-active-learning-for-non-parametric-regression-using-purely-random-trees)|NeurIPS      |2018|
|37  | [Model compression via distillation and quantization](https://openreview.net/forum?id=S1XolQbRW)| ICLR      |2018|
|38  | [The power of deeper networks for expressing natural functions](https://openreview.net/pdf?id=SyProzZAW)|ICLR      |2018|
|39  | [Decision Boundary Analysis of Adversarial Examples](https://openreview.net/pdf?id=BkpiPMbA-)| ICLR      |2018|
|40  | [On the Information Bottleneck Theory of Deep Learning ](https://openreview.net/pdf?id=ry_WPG-A-)|ICLR      |2018|
|41  | [Sensitivity and Generalization in Neural Networks: an Empirical Study ](https://openreview.net/pdf?id=HJC2SzZCW)|ICLR      |2018|
|42  | [Generating Wikipedia by Summarizing Long Sequences](https://openreview.net/pdf?id=Hyg0vbWC-)| ICLR      |2018|
|43  | [Can Neural Networks Understand Logical Entailment?](https://openreview.net/pdf?id=SkZxCk-0Z)| ICLR      |2018|
|44  | [Towards Reverse-Engineering Black-Box Neural Networks](https://openreview.net/pdf?id=BydjJte0-)|ICLR      |2018|
|45  | [The High-Dimensional Geometry of Binary Neural Networks](https://openreview.net/pdf?id=B1IDRdeCW)| ICLR      |2018|
|46  | [Detecting Statistical Interactions from Neural Network Weights ](https://openreview.net/pdf?id=ByOfBggRZ)| ICLR      |2018|
|47  | [The Implicit Bias of Gradient Descent on Separable Data](https://openreview.net/pdf?id=r1q7n9gAb)| ICLR      |2018|
|48  | [Learning how to explain neural networks: PatternNet and PatternAttribution](https://arxiv.org/abs/1705.05598)| ICLR      |2018|
|49  | [GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models ](https://arxiv.org/abs/1802.08773)| ICML      |2018|
|50  | [Which Training Methods for GANs do actually Converge? ](http://proceedings.mlr.press/v80/mescheder18a.html)| ICML      |2018|
|51  | [Nonoverlap-Promoting Variable Selection ](http://proceedings.mlr.press/v80/xie18b.html)| ICML      |2018|
|52  | [An Alternative View: When Does SGD Escape Local Minima? ](http://proceedings.mlr.press/v80/kleinberg18a.html)|ICML      |2018|
|53  | [Stability and Generalization of Learning Algorithms that Converge to Global Optima](http://proceedings.mlr.press/v80/charles18a.html)|ICML      |2018|
|54  | [Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints](http://proceedings.mlr.press/v80/kazemi18a.html)|ICML      |2018|
|55  | [On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization](http://proceedings.mlr.press/v80/arora18a.html)|ICML      |2018|
|56  | [Escaping Saddles with Stochastic Gradients](http://proceedings.mlr.press/v80/daneshmand18a.html)|ICML      |2018|
|57  | [Deep Asymmetric Multi-task Feature Learning](https://arxiv.org/abs/1708.00260)|ICML      |2018|
|58  | [GNN Explainer: A Tool for Post-hoc Explanation of Graph Neural Networks](https://arxiv.org/abs/1903.03894)|     KDD     | 2018   |
